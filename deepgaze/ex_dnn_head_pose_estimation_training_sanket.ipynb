{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_dataset': array([[ 0.8425197 ,  0.6614173 ,  0.6692913 , ..., -0.96850395,\n",
      "        -0.96062994, -1.007874  ],\n",
      "       [ 0.8976378 ,  0.9133858 ,  0.92913383, ..., -0.36220473,\n",
      "        -0.36220473, -0.36220473],\n",
      "       [ 0.78740156,  0.77952754,  0.7637795 , ..., -0.9212598 ,\n",
      "        -0.9133858 , -0.8897638 ],\n",
      "       ...,\n",
      "       [ 0.8110236 ,  0.86614174,  0.81889766, ..., -0.56692916,\n",
      "        -0.5590551 , -0.5590551 ],\n",
      "       [ 0.9527559 , -0.8267717 ,  1.        , ...,  0.8503937 ,\n",
      "         0.8897638 ,  0.9133858 ],\n",
      "       [ 0.37795275,  0.37007874,  0.39370078, ...,  0.6692913 ,\n",
      "         0.68503934, -0.97637796]], dtype=float32), 'training_label': array([[ 38.197186,  28.647888],\n",
      "       [-19.098593, -28.647888],\n",
      "       [-38.197186, -38.197186],\n",
      "       ...,\n",
      "       [  9.549296, -57.295776],\n",
      "       [-38.197186,   9.549296],\n",
      "       [  9.549296,   0.      ]], dtype=float32), 'test_dataset': array([[ 0.8110236 ,  0.86614174,  0.8897638 , ..., -0.43307087,\n",
      "        -0.43307087, -0.43307087],\n",
      "       [-0.48031497, -0.48031497, -0.48031497, ...,  0.62992126,\n",
      "         0.61417323,  0.62992126],\n",
      "       [-0.9448819 , -0.9527559 , -0.97637796, ..., -0.52755904,\n",
      "        -0.51968503, -0.511811  ],\n",
      "       ...,\n",
      "       [ 0.16535433,  0.7559055 , -0.48818898, ...,  0.5590551 ,\n",
      "         0.56692916,  0.5511811 ],\n",
      "       [-0.8582677 , -0.9133858 , -0.93700784, ..., -0.56692916,\n",
      "        -0.56692916, -0.56692916],\n",
      "       [-0.33070865, -0.33070865, -0.33070865, ...,  0.62992126,\n",
      "         0.63779527,  0.6535433 ]], dtype=float32), 'test_label': array([[-19.098593, -38.197186],\n",
      "       [-19.098593,  57.295776],\n",
      "       [-19.098593, -19.098593],\n",
      "       [-38.197186,   0.      ],\n",
      "       [-19.098593,  19.098593],\n",
      "       [ 38.197186,  28.647888],\n",
      "       [  0.      , -28.647888],\n",
      "       [ 38.197186,  -9.549296],\n",
      "       [ 38.197186,   9.549296],\n",
      "       [  9.549296, -57.295776],\n",
      "       [-38.197186,  47.746483],\n",
      "       [  9.549296,  47.746483],\n",
      "       [-38.197186,   9.549296],\n",
      "       [ -9.549296, -38.197186],\n",
      "       [ 19.098593,  19.098593],\n",
      "       [-19.098593,  57.295776],\n",
      "       [-19.098593,  38.197186],\n",
      "       [ -9.549296,   9.549296],\n",
      "       [ -9.549296,  38.197186],\n",
      "       [  9.549296,  57.295776],\n",
      "       [ -9.549296,  38.197186],\n",
      "       [  9.549296, -38.197186],\n",
      "       [  9.549296,  28.647888],\n",
      "       [ -9.549296,  57.295776],\n",
      "       [  9.549296, -57.295776],\n",
      "       [ -9.549296, -19.098593],\n",
      "       [ 19.098593,  38.197186],\n",
      "       [-57.295776,   0.      ],\n",
      "       [ 19.098593,  28.647888],\n",
      "       [ 38.197186,   9.549296],\n",
      "       [ -9.549296, -47.746483],\n",
      "       [ 38.197186,  28.647888],\n",
      "       [ 38.197186,  38.197186],\n",
      "       [ 57.295776,   0.      ],\n",
      "       [  0.      ,  28.647888],\n",
      "       [  9.549296, -47.746483],\n",
      "       [-38.197186,  38.197186],\n",
      "       [ 38.197186, -28.647888],\n",
      "       [ -9.549296,  47.746483],\n",
      "       [-38.197186,  28.647888],\n",
      "       [ 38.197186, -28.647888],\n",
      "       [-38.197186,   0.      ],\n",
      "       [ -9.549296,   9.549296],\n",
      "       [-38.197186, -47.746483],\n",
      "       [ -9.549296,  38.197186],\n",
      "       [  9.549296,  47.746483],\n",
      "       [  9.549296, -19.098593],\n",
      "       [-19.098593,  28.647888],\n",
      "       [ 38.197186, -19.098593],\n",
      "       [-38.197186,  19.098593],\n",
      "       [-19.098593, -19.098593],\n",
      "       [  9.549296,  28.647888],\n",
      "       [  0.      , -47.746483],\n",
      "       [ 19.098593,   9.549296],\n",
      "       [-19.098593,  28.647888],\n",
      "       [-38.197186, -38.197186],\n",
      "       [ 38.197186,  28.647888],\n",
      "       [-19.098593,  28.647888],\n",
      "       [ -9.549296, -19.098593],\n",
      "       [  0.      ,  -9.549296],\n",
      "       [ 38.197186,   0.      ],\n",
      "       [-38.197186, -47.746483],\n",
      "       [  9.549296,   0.      ],\n",
      "       [ 38.197186, -19.098593],\n",
      "       [ -9.549296, -57.295776],\n",
      "       [-38.197186,  19.098593],\n",
      "       [ -9.549296,   0.      ],\n",
      "       [  9.549296, -28.647888],\n",
      "       [ 19.098593,  38.197186],\n",
      "       [  9.549296, -38.197186],\n",
      "       [-38.197186, -38.197186],\n",
      "       [  0.      , -57.295776],\n",
      "       [-38.197186,  -9.549296],\n",
      "       [ 38.197186,  57.295776],\n",
      "       [  0.      ,   9.549296],\n",
      "       [-38.197186, -57.295776],\n",
      "       [ 57.295776,   0.      ],\n",
      "       [-38.197186,  19.098593],\n",
      "       [-19.098593,  38.197186],\n",
      "       [-38.197186, -28.647888],\n",
      "       [-19.098593, -57.295776],\n",
      "       [  0.      , -47.746483],\n",
      "       [ 19.098593,  47.746483],\n",
      "       [-38.197186, -57.295776],\n",
      "       [-19.098593,  38.197186],\n",
      "       [ 19.098593, -28.647888],\n",
      "       [ 19.098593, -28.647888],\n",
      "       [ 19.098593,  -9.549296],\n",
      "       [ -9.549296, -47.746483],\n",
      "       [ 38.197186,  57.295776],\n",
      "       [-38.197186,   0.      ],\n",
      "       [ 19.098593,   0.      ],\n",
      "       [ 57.295776,   0.      ],\n",
      "       [ 19.098593,  38.197186],\n",
      "       [-38.197186, -38.197186],\n",
      "       [ 38.197186,   9.549296],\n",
      "       [-19.098593,  38.197186],\n",
      "       [ 19.098593, -28.647888],\n",
      "       [ 38.197186, -47.746483],\n",
      "       [  0.      ,  57.295776],\n",
      "       [ 38.197186,  47.746483],\n",
      "       [-19.098593,  -9.549296],\n",
      "       [ 38.197186,  57.295776],\n",
      "       [  0.      ,  47.746483],\n",
      "       [-19.098593,  38.197186],\n",
      "       [-19.098593, -38.197186],\n",
      "       [  9.549296,  57.295776],\n",
      "       [ 38.197186, -57.295776],\n",
      "       [-38.197186, -47.746483],\n",
      "       [ 38.197186,  57.295776],\n",
      "       [ 38.197186,  38.197186],\n",
      "       [  9.549296, -28.647888],\n",
      "       [ -9.549296,   9.549296],\n",
      "       [ 19.098593,   0.      ],\n",
      "       [-19.098593,   0.      ],\n",
      "       [ 19.098593,  28.647888],\n",
      "       [-19.098593, -57.295776],\n",
      "       [ -9.549296, -57.295776],\n",
      "       [-19.098593,  -9.549296],\n",
      "       [ 38.197186,  28.647888],\n",
      "       [  9.549296,  28.647888],\n",
      "       [  0.      ,   9.549296],\n",
      "       [-38.197186,  28.647888],\n",
      "       [-38.197186,  38.197186],\n",
      "       [-19.098593,   9.549296],\n",
      "       [  0.      ,   0.      ],\n",
      "       [  0.      , -47.746483],\n",
      "       [  0.      ,  38.197186],\n",
      "       [ -9.549296, -47.746483],\n",
      "       [ 38.197186,  19.098593],\n",
      "       [-57.295776,   0.      ],\n",
      "       [  0.      , -19.098593],\n",
      "       [ -9.549296, -57.295776],\n",
      "       [  0.      , -47.746483],\n",
      "       [ 19.098593,  47.746483],\n",
      "       [  0.      , -19.098593],\n",
      "       [  0.      ,  28.647888],\n",
      "       [ -9.549296, -38.197186],\n",
      "       [  0.      ,  19.098593],\n",
      "       [  9.549296,   0.      ],\n",
      "       [-38.197186, -28.647888],\n",
      "       [ -9.549296,   0.      ],\n",
      "       [-38.197186, -19.098593],\n",
      "       [  9.549296,  -9.549296],\n",
      "       [ -9.549296, -38.197186],\n",
      "       [ 19.098593, -57.295776],\n",
      "       [-38.197186,  38.197186],\n",
      "       [-38.197186, -47.746483],\n",
      "       [-19.098593, -19.098593],\n",
      "       [ 19.098593,   0.      ],\n",
      "       [-19.098593,  -9.549296],\n",
      "       [ 19.098593,  -9.549296],\n",
      "       [ 19.098593, -57.295776],\n",
      "       [ 19.098593, -38.197186],\n",
      "       [ 38.197186, -57.295776],\n",
      "       [ -9.549296, -19.098593],\n",
      "       [  0.      ,  -9.549296],\n",
      "       [ -9.549296,   0.      ],\n",
      "       [-19.098593,  19.098593],\n",
      "       [-19.098593, -19.098593],\n",
      "       [  0.      ,  57.295776],\n",
      "       [ 38.197186,   9.549296],\n",
      "       [ 38.197186,   0.      ],\n",
      "       [ -9.549296, -28.647888],\n",
      "       [-38.197186,   9.549296],\n",
      "       [ -9.549296,  57.295776],\n",
      "       [-19.098593,  57.295776],\n",
      "       [  0.      , -19.098593],\n",
      "       [-19.098593,   0.      ],\n",
      "       [ 19.098593, -19.098593],\n",
      "       [  0.      , -47.746483],\n",
      "       [  0.      ,  -9.549296],\n",
      "       [-38.197186, -57.295776],\n",
      "       [  0.      ,   9.549296],\n",
      "       [-19.098593, -19.098593],\n",
      "       [  9.549296,   0.      ],\n",
      "       [  9.549296,   0.      ],\n",
      "       [ -9.549296, -28.647888],\n",
      "       [  9.549296, -28.647888],\n",
      "       [-19.098593,  19.098593],\n",
      "       [ -9.549296, -38.197186],\n",
      "       [ 19.098593,  57.295776],\n",
      "       [ 19.098593, -57.295776],\n",
      "       [  0.      ,  47.746483],\n",
      "       [  9.549296, -19.098593],\n",
      "       [  9.549296,  57.295776]], dtype=float32)}\n",
      "Training set (2604, 4096) (2604, 2)\n",
      "Test set (186, 4096) (186, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load the standard file\n",
    "pickle_file = 'aflw_dataset.pickle'\n",
    "with open(pickle_file,'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    print(save)\n",
    "    training_dataset = save['training_dataset']\n",
    "    training_label = save['training_label']\n",
    "#     validation_dataset = save['validation_dataset']\n",
    "#     validation_label = save['validation_label']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_label = save['test_label']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', training_dataset.shape, training_label.shape)\n",
    "#     print('Validation set', validation_dataset.shape, validation_label.shape)\n",
    "    print('Test set', test_dataset.shape, test_label.shape)\n",
    "    #Normalising the images\n",
    "    training_dataset -= 127\n",
    "#     validation_dataset -= 127\n",
    "    test_dataset -= 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(predictions, labels):\n",
    "\n",
    "    row, col = labels.shape\n",
    "\n",
    "#     roll_prediction = np.copy(predictions[:,0])\n",
    "    pitch_prediction = np.copy(predictions[:,0])\n",
    "    yaw_prediction = np.copy(predictions[:,1])\n",
    "#     roll_labels = np.copy(labels[:,0])\n",
    "    pitch_labels = np.copy(labels[:,0])\n",
    "    yaw_labels = np.copy(labels[:,1])\n",
    "\n",
    "    #To degree\n",
    "#     roll_prediction *= 25 # to degree\n",
    "    pitch_prediction *= 45 # to degree\n",
    "    yaw_prediction *= 90 # to degree\n",
    "\n",
    "    #Root mean square error\n",
    "#     roll_mean_error = np.sum(np.square(roll_prediction - roll_labels)) * 1/row\n",
    "#     roll_mean_error = np.sqrt(roll_mean_error)\n",
    "    pitch_mean_error = np.sum(np.square(pitch_prediction - pitch_labels)) * 1/row\n",
    "    pitch_mean_error = np.sqrt(pitch_mean_error)\n",
    "    yaw_mean_error = np.sum(np.square(yaw_prediction - yaw_labels)) * 1/row\n",
    "    yaw_mean_error = np.sqrt(yaw_mean_error)\n",
    "\n",
    "    print(\"=== TEST MEAN ERROR (DEGREE) ===\")\n",
    "#     print(\"ROLL: \" + str(roll_mean_error))\n",
    "    print(\"PITCH: \" + str(pitch_mean_error))\n",
    "    print(\"YAW: \" + str(yaw_mean_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels, verbose=False):\n",
    "    #Pitch\n",
    "    N, col = labels.shape\n",
    "    if(verbose == True):\n",
    "        print(\"PRED:   \" + str(predictions[0,:]))\n",
    "        print(\"LABEL:  \" + str(labels[0,:]))\n",
    "\n",
    "        #First value\n",
    "        pred_value = np.copy(predictions[0,2])\n",
    "        real_value = np.copy(labels[0,2])\n",
    "        pred_value *= 25 # to degree\n",
    "        real_value *= 25 # to degree\n",
    "        print(\"=========\")\n",
    "        print(\"PRED PITCH: \" + str(pred_value))\n",
    "        print(\"REAL PITCH: \" + str(real_value))\n",
    "\n",
    "    prediction_copy = np.copy(predictions[:,1])\n",
    "    labels_copy = np.copy(labels[:,1])\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    RMSE = mean_squared_error(labels_copy, prediction_copy)**0.5\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Graph creation...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def multilayer_model(_X, _input0, _biases_input0, _hidden1, _biases_hidden1, _hidden2, _biases_hidden2, _output3, _biases_output3):   \n",
    "    _input0_result = tf.matmul(_X, _input0) + _biases_input0\n",
    "    _hidden1_result = tf.nn.tanh(tf.matmul(_input0_result, _hidden1) + _biases_hidden1)\n",
    "    _hidden2_result = tf.nn.tanh(tf.matmul(_hidden1_result, _hidden2) + _biases_hidden2)\n",
    "    _output3_result = tf.nn.tanh(tf.matmul(_hidden2_result, _output3) + _biases_output3)\n",
    "    return _output3_result\n",
    "\n",
    "batch_size = 128 #was 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    " \n",
    "    print(\"Starting Graph creation...\")\n",
    "    \n",
    "    # Variables\n",
    "    image_size = 64\n",
    "    num_hidden_units_1 = 256 \n",
    "    num_hidden_units_2 = 256 \n",
    "    num_hidden_units_3 = 256 \n",
    "    num_labels = 2\n",
    "    \n",
    "    #0- datasets\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#     tf_valid_dataset = tf.constant(validation_dataset)\n",
    "    #Sanitized version\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #1- weights\n",
    "    #tf.truncated_normal(shape, mean=0.0, stddev=1.0)\n",
    "    weights_input0 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units_1], 0.0, 1.0))    \n",
    "    weights_hidden1 = tf.Variable(tf.truncated_normal([num_hidden_units_1, num_hidden_units_2], 0.0, 1.0))\n",
    "    weights_hidden2 = tf.Variable(tf.truncated_normal([num_hidden_units_2, num_hidden_units_3], 0.0, 1.0))\n",
    "    weights_output3 = tf.Variable(tf.truncated_normal([num_hidden_units_3, num_labels], 0.0, 1.0))    \n",
    "    \n",
    "    #2- biases\n",
    "    biases_input0 = tf.Variable(tf.zeros([num_hidden_units_1]))\n",
    "    biases_hidden1 = tf.Variable(tf.zeros([num_hidden_units_2]))    \n",
    "    biases_hidden2 = tf.Variable(tf.zeros([num_hidden_units_3]))\n",
    "    biases_output3 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    #3- Defining a variable for saving the session parameters\n",
    "    saver = tf.train.Saver({'dnn_weights_input0': weights_input0, \n",
    "                            'dnn_weights_hidden1': weights_hidden1,   \n",
    "                            'dnn_weights_hidden2': weights_hidden2,\n",
    "                            'dnn_weights_output3': weights_output3,\n",
    "                            'dnn_biases_input0': biases_input0,\n",
    "                            'dnn_biases_hidden1': biases_hidden1,\n",
    "                            'dnn_biases_hidden2': biases_hidden2,\n",
    "                            'dnn_biases_output3': biases_output3})   \n",
    "    #4- training\n",
    "    train_prediction = multilayer_model(tf_train_dataset, \n",
    "                     weights_input0, biases_input0, \n",
    "                     weights_hidden1, biases_hidden1, \n",
    "                     weights_hidden2, biases_hidden2, \n",
    "                     weights_output3, biases_output3)\n",
    "    \n",
    "    # Minimize the squared errors.\n",
    "    loss = tf.reduce_mean(tf.square(train_prediction - tf_train_labels))\n",
    "\n",
    "\n",
    "    #5- Adding the regularization terms to the loss\n",
    "    beta = 5e-4\n",
    "    loss += (beta * tf.nn.l2_loss(weights_input0)) \n",
    "    loss += (beta * tf.nn.l2_loss(weights_hidden1)) \n",
    "    loss += (beta * tf.nn.l2_loss(weights_hidden2)) \n",
    "    loss += (beta * tf.nn.l2_loss(weights_output3))\n",
    "    \n",
    "    #6- Optimizer.\n",
    "#     learning_rate = 0.001\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) \n",
    "        \n",
    "    #Valid prediction\n",
    "#     valid_prediction = multilayer_model(tf_valid_dataset, \n",
    "#                      weights_input0, biases_input0, \n",
    "#                      weights_hidden1, biases_hidden1, \n",
    "#                      weights_hidden2, biases_hidden2, \n",
    "#                      weights_output3, biases_output3)\n",
    "    #Test prediction\n",
    "    test_prediction = multilayer_model(tf_test_dataset, \n",
    "                     weights_input0, biases_input0, \n",
    "                     weights_hidden1, biases_hidden1, \n",
    "                     weights_hidden2, biases_hidden2, \n",
    "                     weights_output3, biases_output3)\n",
    "    \n",
    "    print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Train dataset shape:  (2604, 4096)\n",
      "Minibatch loss at step 0: 1186.357788\n",
      "Minibatch accuracy: 35.16198217157808\n",
      "Minibatch loss at step 100: 1158.364868\n",
      "Minibatch accuracy: 36.53272358554896\n",
      "Minibatch loss at step 200: 1238.864868\n",
      "Minibatch accuracy: 36.526280805101486\n",
      "Minibatch loss at step 300: 1036.407593\n",
      "Minibatch accuracy: 33.65081272815896\n",
      "Minibatch loss at step 400: 1099.081543\n",
      "Minibatch accuracy: 35.19256457779825\n",
      "Minibatch loss at step 500: 1130.342529\n",
      "Minibatch accuracy: 35.63471975409832\n",
      "Minibatch loss at step 600: 1074.057739\n",
      "Minibatch accuracy: 33.72118634038733\n",
      "Minibatch loss at step 700: 1112.431641\n",
      "Minibatch accuracy: 35.627112396253956\n",
      "Minibatch loss at step 800: 1173.412964\n",
      "Minibatch accuracy: 36.79924023714688\n",
      "Minibatch loss at step 900: 1083.858154\n",
      "Minibatch accuracy: 36.496351085802054\n",
      "Minibatch loss at step 1000: 1161.841064\n",
      "Minibatch accuracy: 37.48235913191364\n",
      "Minibatch loss at step 1100: 1019.195374\n",
      "Minibatch accuracy: 33.72218182257785\n",
      "Minibatch loss at step 1200: 1012.925049\n",
      "Minibatch accuracy: 34.1275575678494\n",
      "Minibatch loss at step 1300: 1104.814453\n",
      "Minibatch accuracy: 36.194643886820465\n",
      "Minibatch loss at step 1400: 1143.070679\n",
      "Minibatch accuracy: 37.83632062189881\n",
      "Minibatch loss at step 1500: 993.740906\n",
      "Minibatch accuracy: 35.05614741639029\n",
      "Minibatch loss at step 1600: 1069.913086\n",
      "Minibatch accuracy: 37.29175413892209\n",
      "Minibatch loss at step 1700: 1018.456421\n",
      "Minibatch accuracy: 34.68799971486897\n",
      "Minibatch loss at step 1800: 1011.776917\n",
      "Minibatch accuracy: 35.841004962455365\n",
      "Minibatch loss at step 1900: 1074.461060\n",
      "Minibatch accuracy: 35.60805518067506\n",
      "Minibatch loss at step 2000: 1059.548828\n",
      "Minibatch accuracy: 36.3397965121831\n",
      "Minibatch loss at step 2100: 991.718323\n",
      "Minibatch accuracy: 35.509831316371915\n",
      "Minibatch loss at step 2200: 917.408875\n",
      "Minibatch accuracy: 35.49722494105173\n",
      "Minibatch loss at step 2300: 994.539978\n",
      "Minibatch accuracy: 33.67441260739562\n",
      "Minibatch loss at step 2400: 920.555054\n",
      "Minibatch accuracy: 35.21846924799366\n",
      "Minibatch loss at step 2500: 1067.170410\n",
      "Minibatch accuracy: 35.93832200146871\n",
      "Minibatch loss at step 2600: 987.682861\n",
      "Minibatch accuracy: 34.34032519604776\n",
      "Minibatch loss at step 2700: 1001.748962\n",
      "Minibatch accuracy: 36.022662341204565\n",
      "Minibatch loss at step 2800: 911.284119\n",
      "Minibatch accuracy: 35.29029999075525\n",
      "Minibatch loss at step 2900: 1025.612183\n",
      "Minibatch accuracy: 34.63015389214651\n",
      "Minibatch loss at step 3000: 852.183044\n",
      "Minibatch accuracy: 32.47221242967785\n",
      "Minibatch loss at step 3100: 1044.669434\n",
      "Minibatch accuracy: 36.12408762904954\n",
      "Minibatch loss at step 3200: 847.386597\n",
      "Minibatch accuracy: 31.503105858167977\n",
      "Minibatch loss at step 3300: 1013.932861\n",
      "Minibatch accuracy: 36.5010133367469\n",
      "Minibatch loss at step 3400: 932.796692\n",
      "Minibatch accuracy: 35.254612397706204\n",
      "Minibatch loss at step 3500: 1000.468201\n",
      "Minibatch accuracy: 34.08239098126773\n",
      "Minibatch loss at step 3600: 886.514404\n",
      "Minibatch accuracy: 33.16582463663651\n",
      "Minibatch loss at step 3700: 984.859802\n",
      "Minibatch accuracy: 34.79385967233928\n",
      "Minibatch loss at step 3800: 802.834900\n",
      "Minibatch accuracy: 31.46472421027882\n",
      "Minibatch loss at step 3900: 1033.137573\n",
      "Minibatch accuracy: 36.795698959444465\n",
      "Minibatch loss at step 4000: 971.395813\n",
      "Minibatch accuracy: 36.18552658155433\n",
      "Minibatch loss at step 4100: 958.013489\n",
      "Minibatch accuracy: 36.321748432305505\n",
      "Minibatch loss at step 4200: 969.565735\n",
      "Minibatch accuracy: 34.71185976014268\n",
      "Minibatch loss at step 4300: 912.865295\n",
      "Minibatch accuracy: 34.02246787510424\n",
      "Minibatch loss at step 4400: 815.226562\n",
      "Minibatch accuracy: 32.68630494947387\n",
      "Minibatch loss at step 4500: 958.317078\n",
      "Minibatch accuracy: 34.62196088873722\n",
      "Minibatch loss at step 4600: 1021.195984\n",
      "Minibatch accuracy: 37.52382706569587\n",
      "Minibatch loss at step 4700: 871.006653\n",
      "Minibatch accuracy: 35.41554724027464\n",
      "Minibatch loss at step 4800: 1050.299316\n",
      "Minibatch accuracy: 37.505282831014284\n",
      "Minibatch loss at step 4900: 972.075623\n",
      "Minibatch accuracy: 35.63920358301024\n",
      "Minibatch loss at step 5000: 840.118958\n",
      "Minibatch accuracy: 33.110396250177224\n",
      "Minibatch loss at step 5100: 873.504150\n",
      "Minibatch accuracy: 33.09280938819074\n",
      "Minibatch loss at step 5200: 993.336426\n",
      "Minibatch accuracy: 37.349976599097424\n",
      "Minibatch loss at step 5300: 857.907043\n",
      "Minibatch accuracy: 35.63194833555274\n",
      "Minibatch loss at step 5400: 1011.024231\n",
      "Minibatch accuracy: 36.96192416380166\n",
      "Minibatch loss at step 5500: 1022.717346\n",
      "Minibatch accuracy: 36.62998246292749\n",
      "Minibatch loss at step 5600: 897.461304\n",
      "Minibatch accuracy: 34.41423187387378\n",
      "Minibatch loss at step 5700: 905.534973\n",
      "Minibatch accuracy: 34.570329272594805\n",
      "Minibatch loss at step 5800: 977.031433\n",
      "Minibatch accuracy: 36.23630701781105\n",
      "Minibatch loss at step 5900: 941.073364\n",
      "Minibatch accuracy: 34.4488376017156\n",
      "Minibatch loss at step 6000: 992.244812\n",
      "Minibatch accuracy: 36.401448181185046\n",
      "Minibatch loss at step 6100: 1048.289185\n",
      "Minibatch accuracy: 36.93223180493903\n",
      "Minibatch loss at step 6200: 898.948181\n",
      "Minibatch accuracy: 35.23016535273784\n",
      "Minibatch loss at step 6300: 997.286804\n",
      "Minibatch accuracy: 37.00726741866893\n",
      "Minibatch loss at step 6400: 942.475403\n",
      "Minibatch accuracy: 34.69902328303889\n",
      "Minibatch loss at step 6500: 958.076233\n",
      "Minibatch accuracy: 35.4318055113887\n",
      "Minibatch loss at step 6600: 999.811707\n",
      "Minibatch accuracy: 36.69561438693151\n",
      "Minibatch loss at step 6700: 1045.410889\n",
      "Minibatch accuracy: 37.67747068043448\n",
      "Minibatch loss at step 6800: 923.563232\n",
      "Minibatch accuracy: 36.20236296133686\n",
      "Minibatch loss at step 6900: 993.331360\n",
      "Minibatch accuracy: 37.02469615000236\n",
      "Minibatch loss at step 7000: 928.943298\n",
      "Minibatch accuracy: 34.326430531120074\n",
      "Minibatch loss at step 7100: 915.239441\n",
      "Minibatch accuracy: 35.231177099889095\n",
      "Minibatch loss at step 7200: 954.959229\n",
      "Minibatch accuracy: 35.15856936793646\n",
      "Minibatch loss at step 7300: 1022.596008\n",
      "Minibatch accuracy: 36.81225793108017\n",
      "Minibatch loss at step 7400: 900.181458\n",
      "Minibatch accuracy: 35.37559179882631\n",
      "Minibatch loss at step 7500: 883.556396\n",
      "Minibatch accuracy: 35.38830186911612\n",
      "Minibatch loss at step 7600: 957.120361\n",
      "Minibatch accuracy: 34.87186715879471\n",
      "Minibatch loss at step 7700: 889.321411\n",
      "Minibatch accuracy: 35.59512864977103\n",
      "Minibatch loss at step 7800: 962.737305\n",
      "Minibatch accuracy: 35.582904131144915\n",
      "Minibatch loss at step 7900: 985.257202\n",
      "Minibatch accuracy: 35.99120477684878\n",
      "Minibatch loss at step 8000: 936.527588\n",
      "Minibatch accuracy: 35.62345631249777\n",
      "Minibatch loss at step 8100: 848.211243\n",
      "Minibatch accuracy: 35.289822640971856\n",
      "Minibatch loss at step 8200: 949.993408\n",
      "Minibatch accuracy: 33.78400702787304\n",
      "Minibatch loss at step 8300: 827.334412\n",
      "Minibatch accuracy: 33.55525410610088\n",
      "Minibatch loss at step 8400: 971.490601\n",
      "Minibatch accuracy: 35.69588445454444\n",
      "Minibatch loss at step 8500: 861.556763\n",
      "Minibatch accuracy: 32.82584364098184\n",
      "Minibatch loss at step 8600: 978.414062\n",
      "Minibatch accuracy: 36.837195928928274\n",
      "Minibatch loss at step 8700: 865.179688\n",
      "Minibatch accuracy: 34.96959826384019\n",
      "Minibatch loss at step 8800: 985.567688\n",
      "Minibatch accuracy: 34.93695144359157\n",
      "Minibatch loss at step 8900: 806.257080\n",
      "Minibatch accuracy: 32.230500706337935\n",
      "Minibatch loss at step 9000: 972.280518\n",
      "Minibatch accuracy: 35.48786826431411\n",
      "Minibatch loss at step 9100: 800.691101\n",
      "Minibatch accuracy: 32.17182946815622\n",
      "Minibatch loss at step 9200: 965.436646\n",
      "Minibatch accuracy: 36.455559418283514\n",
      "Minibatch loss at step 9300: 898.151123\n",
      "Minibatch accuracy: 35.33392256719278\n",
      "Minibatch loss at step 9400: 963.605042\n",
      "Minibatch accuracy: 35.38040346319909\n",
      "Minibatch loss at step 9500: 944.839478\n",
      "Minibatch accuracy: 34.4988270435795\n",
      "Minibatch loss at step 9600: 945.929749\n",
      "Minibatch accuracy: 35.05051460704607\n",
      "Minibatch loss at step 9700: 801.311829\n",
      "Minibatch accuracy: 33.34869559738244\n",
      "Minibatch loss at step 9800: 960.636841\n",
      "Minibatch accuracy: 35.59134923861415\n",
      "Minibatch loss at step 9900: 926.733582\n",
      "Minibatch accuracy: 36.340375958493986\n",
      "Minibatch loss at step 10000: 888.415039\n",
      "Minibatch accuracy: 35.87564139507212\n",
      "Minibatch loss at step 10100: 1009.364990\n",
      "Minibatch accuracy: 36.87220924291254\n",
      "Minibatch loss at step 10200: 914.488342\n",
      "Minibatch accuracy: 34.66745620219704\n",
      "Minibatch loss at step 10300: 793.986755\n",
      "Minibatch accuracy: 32.63457152758502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 10400: 856.795349\n",
      "Minibatch accuracy: 32.90800832196853\n",
      "Minibatch loss at step 10500: 1033.414795\n",
      "Minibatch accuracy: 38.59258127025922\n",
      "Minibatch loss at step 10600: 853.577515\n",
      "Minibatch accuracy: 35.893209800262014\n",
      "Minibatch loss at step 10700: 1021.399292\n",
      "Minibatch accuracy: 37.540568875863315\n",
      "Minibatch loss at step 10800: 989.426086\n",
      "Minibatch accuracy: 35.834379905380324\n",
      "Minibatch loss at step 10900: 882.036072\n",
      "Minibatch accuracy: 34.43633377675932\n",
      "Minibatch loss at step 11000: 883.674988\n",
      "Minibatch accuracy: 33.694126928192176\n",
      "Minibatch loss at step 11100: 1042.419678\n",
      "Minibatch accuracy: 38.09090111886138\n",
      "Minibatch loss at step 11200: 850.765442\n",
      "Minibatch accuracy: 35.500598311419346\n",
      "Minibatch loss at step 11300: 940.761230\n",
      "Minibatch accuracy: 36.428688096145464\n",
      "Minibatch loss at step 11400: 1032.203369\n",
      "Minibatch accuracy: 37.010694453174615\n",
      "Minibatch loss at step 11500: 852.919739\n",
      "Minibatch accuracy: 33.71083701462428\n",
      "Minibatch loss at step 11600: 952.142273\n",
      "Minibatch accuracy: 35.96047838752114\n",
      "Minibatch loss at step 11700: 967.436340\n",
      "Minibatch accuracy: 35.874292240405246\n",
      "Minibatch loss at step 11800: 899.217529\n",
      "Minibatch accuracy: 33.81292239223231\n",
      "Minibatch loss at step 11900: 965.735046\n",
      "Minibatch accuracy: 35.811264362954816\n",
      "Minibatch loss at step 12000: 1014.706482\n",
      "Minibatch accuracy: 36.92551822905638\n",
      "Minibatch loss at step 12100: 940.305359\n",
      "Minibatch accuracy: 36.480916926588996\n",
      "Minibatch loss at step 12200: 1016.164124\n",
      "Minibatch accuracy: 37.2349061019613\n",
      "Minibatch loss at step 12300: 882.467224\n",
      "Minibatch accuracy: 33.58186199540051\n",
      "Minibatch loss at step 12400: 918.723328\n",
      "Minibatch accuracy: 34.96346796009908\n",
      "Minibatch loss at step 12500: 1006.552002\n",
      "Minibatch accuracy: 36.80243787568631\n",
      "Minibatch loss at step 12600: 1033.450562\n",
      "Minibatch accuracy: 38.188932051708385\n",
      "Minibatch loss at step 12700: 866.272827\n",
      "Minibatch accuracy: 35.185881609651865\n",
      "Minibatch loss at step 12800: 964.574219\n",
      "Minibatch accuracy: 36.936784508193014\n",
      "Minibatch loss at step 12900: 909.087952\n",
      "Minibatch accuracy: 34.57750013556503\n",
      "Minibatch loss at step 13000: 900.100281\n",
      "Minibatch accuracy: 35.282260282257994\n",
      "Minibatch loss at step 13100: 961.634338\n",
      "Minibatch accuracy: 34.94404881881266\n",
      "Minibatch loss at step 13200: 1011.106323\n",
      "Minibatch accuracy: 37.243822238350816\n",
      "Minibatch loss at step 13300: 909.932007\n",
      "Minibatch accuracy: 35.44627247570434\n",
      "Minibatch loss at step 13400: 873.550537\n",
      "Minibatch accuracy: 36.068545041438114\n",
      "Minibatch loss at step 13500: 921.142029\n",
      "Minibatch accuracy: 33.853171660057384\n",
      "Minibatch loss at step 13600: 873.172058\n",
      "Minibatch accuracy: 35.87968175026083\n",
      "Minibatch loss at step 13700: 975.698669\n",
      "Minibatch accuracy: 35.694118119928035\n",
      "Minibatch loss at step 13800: 958.393433\n",
      "Minibatch accuracy: 35.513677836526064\n",
      "Minibatch loss at step 13900: 924.038940\n",
      "Minibatch accuracy: 35.169386434618104\n",
      "Minibatch loss at step 14000: 848.463989\n",
      "Minibatch accuracy: 35.09682825435405\n",
      "Minibatch loss at step 14100: 988.139587\n",
      "Minibatch accuracy: 34.937852888423535\n",
      "Minibatch loss at step 14200: 810.402344\n",
      "Minibatch accuracy: 32.88743675164956\n",
      "Minibatch loss at step 14300: 974.860229\n",
      "Minibatch accuracy: 35.72936672380823\n",
      "Minibatch loss at step 14400: 811.759216\n",
      "Minibatch accuracy: 31.853640591701797\n",
      "Minibatch loss at step 14500: 981.910950\n",
      "Minibatch accuracy: 36.909408602135244\n",
      "Minibatch loss at step 14600: 889.321045\n",
      "Minibatch accuracy: 35.52054313175687\n",
      "Minibatch loss at step 14700: 968.358093\n",
      "Minibatch accuracy: 34.29746048759933\n",
      "Minibatch loss at step 14800: 829.644775\n",
      "Minibatch accuracy: 32.73044427474022\n",
      "Minibatch loss at step 14900: 956.891541\n",
      "Minibatch accuracy: 35.02784188372515\n",
      "Minibatch loss at step 15000: 761.366577\n",
      "Minibatch accuracy: 31.330725811618418\n",
      "Minibatch loss at step 15100: 1021.129883\n",
      "Minibatch accuracy: 37.24286188947823\n",
      "Minibatch loss at step 15200: 932.215942\n",
      "Minibatch accuracy: 35.9862424206069\n",
      "Minibatch loss at step 15300: 910.455933\n",
      "Minibatch accuracy: 35.50136681732744\n",
      "Minibatch loss at step 15400: 957.685852\n",
      "Minibatch accuracy: 34.681819636248186\n",
      "Minibatch loss at step 15500: 895.386353\n",
      "Minibatch accuracy: 34.33567530474408\n",
      "Minibatch loss at step 15600: 792.294678\n",
      "Minibatch accuracy: 32.5389977865503\n",
      "Minibatch loss at step 15700: 942.839539\n",
      "Minibatch accuracy: 35.00803827895281\n",
      "Minibatch loss at step 15800: 990.315674\n",
      "Minibatch accuracy: 37.44511282676786\n",
      "Minibatch loss at step 15900: 851.154419\n",
      "Minibatch accuracy: 35.24698014546778\n",
      "Minibatch loss at step 16000: 1033.132202\n",
      "Minibatch accuracy: 37.64806381454928\n",
      "Minibatch loss at step 16100: 959.225586\n",
      "Minibatch accuracy: 35.62168467303747\n",
      "Minibatch loss at step 16200: 798.028381\n",
      "Minibatch accuracy: 32.532343806659355\n",
      "Minibatch loss at step 16300: 866.165588\n",
      "Minibatch accuracy: 33.32385770266552\n",
      "Minibatch loss at step 16400: 965.182800\n",
      "Minibatch accuracy: 37.293385886231555\n",
      "Minibatch loss at step 16500: 818.654907\n",
      "Minibatch accuracy: 35.33618190966767\n",
      "Minibatch loss at step 16600: 1007.577393\n",
      "Minibatch accuracy: 37.11851532170299\n",
      "Minibatch loss at step 16700: 1009.767578\n",
      "Minibatch accuracy: 36.60668404002539\n",
      "Minibatch loss at step 16800: 879.848267\n",
      "Minibatch accuracy: 34.40885938958436\n",
      "Minibatch loss at step 16900: 872.075623\n",
      "Minibatch accuracy: 33.89858064183883\n",
      "Minibatch loss at step 17000: 965.493408\n",
      "Minibatch accuracy: 36.35904765914101\n",
      "Minibatch loss at step 17100: 907.688110\n",
      "Minibatch accuracy: 34.27478289045023\n",
      "Minibatch loss at step 17200: 983.023071\n",
      "Minibatch accuracy: 36.70099803596542\n",
      "Minibatch loss at step 17300: 1057.269897\n",
      "Minibatch accuracy: 37.471312269042066\n",
      "Minibatch loss at step 17400: 873.005554\n",
      "Minibatch accuracy: 34.86427366409632\n",
      "Minibatch loss at step 17500: 959.196594\n",
      "Minibatch accuracy: 36.214725563729594\n",
      "Minibatch loss at step 17600: 918.863892\n",
      "Minibatch accuracy: 34.72233259531069\n",
      "Minibatch loss at step 17700: 930.117676\n",
      "Minibatch accuracy: 34.795306851732896\n",
      "Minibatch loss at step 17800: 962.662415\n",
      "Minibatch accuracy: 35.9890459137395\n",
      "Minibatch loss at step 17900: 1049.802979\n",
      "Minibatch accuracy: 38.01867056491373\n",
      "Minibatch loss at step 18000: 934.248230\n",
      "Minibatch accuracy: 36.67585260171193\n",
      "Minibatch loss at step 18100: 978.604675\n",
      "Minibatch accuracy: 36.82447379280779\n",
      "Minibatch loss at step 18200: 938.358704\n",
      "Minibatch accuracy: 34.59955595756884\n",
      "Minibatch loss at step 18300: 930.957642\n",
      "Minibatch accuracy: 35.52468400743524\n",
      "Minibatch loss at step 18400: 956.185120\n",
      "Minibatch accuracy: 35.59512864977103\n",
      "Minibatch loss at step 18500: 1039.633423\n",
      "Minibatch accuracy: 37.541126536648115\n",
      "Minibatch loss at step 18600: 904.593384\n",
      "Minibatch accuracy: 35.67792292959769\n",
      "Minibatch loss at step 18700: 897.445862\n",
      "Minibatch accuracy: 35.7914235673375\n",
      "Minibatch loss at step 18800: 942.506592\n",
      "Minibatch accuracy: 34.757660604579314\n",
      "Minibatch loss at step 18900: 872.166931\n",
      "Minibatch accuracy: 35.400888412448424\n",
      "Minibatch loss at step 19000: 950.150513\n",
      "Minibatch accuracy: 35.63843633678742\n",
      "Minibatch loss at step 19100: 993.803955\n",
      "Minibatch accuracy: 36.53329997188781\n",
      "Minibatch loss at step 19200: 931.562195\n",
      "Minibatch accuracy: 35.90562105084488\n",
      "Minibatch loss at step 19300: 851.981079\n",
      "Minibatch accuracy: 35.348015188046624\n",
      "Minibatch loss at step 19400: 928.326538\n",
      "Minibatch accuracy: 33.70104957100743\n",
      "Minibatch loss at step 19500: 824.553040\n",
      "Minibatch accuracy: 33.86676308125066\n",
      "Minibatch loss at step 19600: 968.195740\n",
      "Minibatch accuracy: 35.3789784920456\n",
      "Minibatch loss at step 19700: 860.467773\n",
      "Minibatch accuracy: 33.000157211391624\n",
      "Minibatch loss at step 19800: 953.798401\n",
      "Minibatch accuracy: 36.39088161405873\n",
      "Minibatch loss at step 19900: 862.328003\n",
      "Minibatch accuracy: 35.256044126659155\n",
      "Minibatch loss at step 20000: 965.643311\n",
      "Minibatch accuracy: 34.654055526779615\n",
      "Minibatch loss at step 20100: 806.819824\n",
      "Minibatch accuracy: 32.59345974523205\n",
      "Minibatch loss at step 20200: 978.159607\n",
      "Minibatch accuracy: 35.535448025967675\n",
      "Minibatch loss at step 20300: 794.338745\n",
      "Minibatch accuracy: 32.02064611065414\n",
      "Minibatch loss at step 20400: 954.196106\n",
      "Minibatch accuracy: 36.20607859121345\n",
      "Minibatch loss at step 20500: 896.124023\n",
      "Minibatch accuracy: 35.36632202248966\n",
      "Minibatch loss at step 20600: 940.240479\n",
      "Minibatch accuracy: 34.66260719984325\n",
      "Minibatch loss at step 20700: 938.449524\n",
      "Minibatch accuracy: 34.58947297408563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 20800: 911.375122\n",
      "Minibatch accuracy: 34.40360670477552\n",
      "Minibatch loss at step 20900: 786.548828\n",
      "Minibatch accuracy: 32.9865251253258\n",
      "Minibatch loss at step 21000: 954.878113\n",
      "Minibatch accuracy: 35.63230119809658\n",
      "Minibatch loss at step 21100: 939.831177\n",
      "Minibatch accuracy: 36.622410184672106\n",
      "Minibatch loss at step 21200: 887.656677\n",
      "Minibatch accuracy: 35.56896291778656\n",
      "Minibatch loss at step 21300: 977.618469\n",
      "Minibatch accuracy: 36.19876497806935\n",
      "Minibatch loss at step 21400: 911.162354\n",
      "Minibatch accuracy: 34.52427991023582\n",
      "Minibatch loss at step 21500: 807.843567\n",
      "Minibatch accuracy: 32.6619929908267\n",
      "Minibatch loss at step 21600: 882.567444\n",
      "Minibatch accuracy: 33.4251729614074\n",
      "Minibatch loss at step 21700: 1009.884155\n",
      "Minibatch accuracy: 37.88189332381468\n",
      "Minibatch loss at step 21800: 831.993347\n",
      "Minibatch accuracy: 35.382126808050096\n",
      "Minibatch loss at step 21900: 1018.797607\n",
      "Minibatch accuracy: 37.35475942577883\n",
      "Minibatch loss at step 22000: 990.362549\n",
      "Minibatch accuracy: 35.785493743973966\n",
      "Minibatch loss at step 22100: 867.031311\n",
      "Minibatch accuracy: 34.14258075061843\n",
      "Minibatch loss at step 22200: 912.525757\n",
      "Minibatch accuracy: 34.424307676181016\n",
      "Minibatch loss at step 22300: 1021.822327\n",
      "Minibatch accuracy: 37.67440401351344\n",
      "Minibatch loss at step 22400: 846.862366\n",
      "Minibatch accuracy: 35.37651484720301\n",
      "Minibatch loss at step 22500: 949.591309\n",
      "Minibatch accuracy: 36.52179558475151\n",
      "Minibatch loss at step 22600: 1027.353027\n",
      "Minibatch accuracy: 37.17904479771249\n",
      "Minibatch loss at step 22700: 859.125610\n",
      "Minibatch accuracy: 33.58874235932703\n",
      "Minibatch loss at step 22800: 918.505493\n",
      "Minibatch accuracy: 35.435124823952044\n",
      "Minibatch loss at step 22900: 991.322571\n",
      "Minibatch accuracy: 36.39284557415613\n",
      "Minibatch loss at step 23000: 914.041016\n",
      "Minibatch accuracy: 34.24816087671716\n",
      "Minibatch loss at step 23100: 978.569885\n",
      "Minibatch accuracy: 36.0524621807842\n",
      "Minibatch loss at step 23200: 1032.192139\n",
      "Minibatch accuracy: 36.97170185805963\n",
      "Minibatch loss at step 23300: 911.827576\n",
      "Minibatch accuracy: 36.03911076709319\n",
      "Minibatch loss at step 23400: 1018.992310\n",
      "Minibatch accuracy: 37.31445634527924\n",
      "Minibatch loss at step 23500: 880.102966\n",
      "Minibatch accuracy: 33.53545632691894\n",
      "Minibatch loss at step 23600: 905.762695\n",
      "Minibatch accuracy: 34.824346475158364\n",
      "Minibatch loss at step 23700: 993.566467\n",
      "Minibatch accuracy: 36.449622099164145\n",
      "Minibatch loss at step 23800: 1021.895386\n",
      "Minibatch accuracy: 38.24529082786288\n",
      "Minibatch loss at step 23900: 885.488281\n",
      "Minibatch accuracy: 35.87552400531942\n",
      "Minibatch loss at step 24000: 971.629883\n",
      "Minibatch accuracy: 37.10826643081027\n",
      "Minibatch loss at step 24100: 912.033569\n",
      "Minibatch accuracy: 34.44499620658819\n",
      "Minibatch loss at step 24200: 894.471741\n",
      "Minibatch accuracy: 34.94645387835331\n",
      "Minibatch loss at step 24300: 967.018860\n",
      "Minibatch accuracy: 35.05950577678613\n",
      "Minibatch loss at step 24400: 986.628967\n",
      "Minibatch accuracy: 36.485870552859225\n",
      "Minibatch loss at step 24500: 879.548950\n",
      "Minibatch accuracy: 34.82289874987075\n",
      "Minibatch loss at step 24600: 846.978394\n",
      "Minibatch accuracy: 35.377075565766276\n",
      "Minibatch loss at step 24700: 909.390137\n",
      "Minibatch accuracy: 33.32177696811051\n",
      "Minibatch loss at step 24800: 866.911011\n",
      "Minibatch accuracy: 35.880268627169066\n",
      "Minibatch loss at step 24900: 963.579651\n",
      "Minibatch accuracy: 35.4383267099337\n",
      "Minibatch loss at step 25000: 961.771362\n",
      "Minibatch accuracy: 35.34319911429655\n",
      "Minibatch loss at step 25100: 887.895569\n",
      "Minibatch accuracy: 34.48688467872606\n",
      "Minibatch loss at step 25200: 850.201904\n",
      "Minibatch accuracy: 35.112872544970955\n",
      "Minibatch loss at step 25300: 997.477478\n",
      "Minibatch accuracy: 34.89421793524681\n",
      "Minibatch loss at step 25400: 797.914368\n",
      "Minibatch accuracy: 32.486513127084756\n",
      "Minibatch loss at step 25500: 989.983826\n",
      "Minibatch accuracy: 35.667142015183245\n",
      "Minibatch loss at step 25600: 809.210449\n",
      "Minibatch accuracy: 31.475742264070895\n",
      "Minibatch loss at step 25700: 998.068848\n",
      "Minibatch accuracy: 37.04081166113433\n",
      "Minibatch loss at step 25800: 878.561157\n",
      "Minibatch accuracy: 35.38265983765918\n",
      "Minibatch loss at step 25900: 978.601074\n",
      "Minibatch accuracy: 34.67751651094968\n",
      "Minibatch loss at step 26000: 829.104797\n",
      "Minibatch accuracy: 32.56694952205188\n",
      "Minibatch loss at step 26100: 985.354126\n",
      "Minibatch accuracy: 35.54520257054741\n",
      "Minibatch loss at step 26200: 766.858704\n",
      "Minibatch accuracy: 31.347623880435002\n",
      "Minibatch loss at step 26300: 1009.942566\n",
      "Minibatch accuracy: 36.96055851501959\n",
      "Minibatch loss at step 26400: 925.975891\n",
      "Minibatch accuracy: 35.99178813947096\n",
      "Minibatch loss at step 26500: 923.028992\n",
      "Minibatch accuracy: 35.40237456725184\n",
      "Minibatch loss at step 26600: 952.052551\n",
      "Minibatch accuracy: 34.53815683188305\n",
      "Minibatch loss at step 26700: 908.907776\n",
      "Minibatch accuracy: 34.48919950959286\n",
      "Minibatch loss at step 26800: 790.117065\n",
      "Minibatch accuracy: 32.430379155777686\n",
      "Minibatch loss at step 26900: 954.900757\n",
      "Minibatch accuracy: 35.16514470813038\n",
      "Minibatch loss at step 27000: 984.590515\n",
      "Minibatch accuracy: 37.26548250758921\n",
      "Minibatch loss at step 27100: 854.470398\n",
      "Minibatch accuracy: 35.17990176622513\n",
      "Minibatch loss at step 27200: 1029.009521\n",
      "Minibatch accuracy: 37.46772374011564\n",
      "Minibatch loss at step 27300: 930.126343\n",
      "Minibatch accuracy: 34.97759118014883\n",
      "Minibatch loss at step 27400: 790.082886\n",
      "Minibatch accuracy: 32.60313039154561\n",
      "Minibatch loss at step 27500: 850.705017\n",
      "Minibatch accuracy: 33.04171428380344\n",
      "Minibatch loss at step 27600: 997.235046\n",
      "Minibatch accuracy: 37.673284528794404\n",
      "Minibatch loss at step 27700: 856.042175\n",
      "Minibatch accuracy: 36.27751851504421\n",
      "Minibatch loss at step 27800: 1001.502563\n",
      "Minibatch accuracy: 36.82316105971468\n",
      "Minibatch loss at step 27900: 989.384888\n",
      "Minibatch accuracy: 36.14554758682682\n",
      "Minibatch loss at step 28000: 878.189087\n",
      "Minibatch accuracy: 34.46329392164492\n",
      "Minibatch loss at step 28100: 833.951965\n",
      "Minibatch accuracy: 33.10887358044101\n",
      "Minibatch loss at step 28200: 978.975159\n",
      "Minibatch accuracy: 36.830407027834184\n",
      "Minibatch loss at step 28300: 928.654114\n",
      "Minibatch accuracy: 34.88138732390994\n",
      "Minibatch loss at step 28400: 957.217773\n",
      "Minibatch accuracy: 36.43151282822503\n",
      "Minibatch loss at step 28500: 1045.002075\n",
      "Minibatch accuracy: 37.1291000208647\n",
      "Minibatch loss at step 28600: 854.492981\n",
      "Minibatch accuracy: 34.60477889910944\n",
      "Minibatch loss at step 28700: 940.881958\n",
      "Minibatch accuracy: 35.835902584733375\n",
      "Minibatch loss at step 28800: 950.908020\n",
      "Minibatch accuracy: 35.375709122199055\n",
      "Minibatch loss at step 28900: 914.154419\n",
      "Minibatch accuracy: 34.455550155576354\n",
      "Minibatch loss at step 29000: 940.229797\n",
      "Minibatch accuracy: 35.54195364284299\n",
      "Minibatch loss at step 29100: 1027.291382\n",
      "Minibatch accuracy: 37.44326111244018\n",
      "Minibatch loss at step 29200: 929.175476\n",
      "Minibatch accuracy: 36.61236583493684\n",
      "Minibatch loss at step 29300: 970.138062\n",
      "Minibatch accuracy: 36.65728232774833\n",
      "Minibatch loss at step 29400: 954.763123\n",
      "Minibatch accuracy: 34.79814665671417\n",
      "Minibatch loss at step 29500: 919.341980\n",
      "Minibatch accuracy: 35.127533410639934\n",
      "Minibatch loss at step 29600: 946.426147\n",
      "Minibatch accuracy: 35.40136426462792\n",
      "Minibatch loss at step 29700: 1047.105713\n",
      "Minibatch accuracy: 37.84681745929075\n",
      "Minibatch loss at step 29800: 892.458801\n",
      "Minibatch accuracy: 35.33761033377275\n",
      "Minibatch loss at step 29900: 924.393372\n",
      "Minibatch accuracy: 36.363672124671695\n",
      "Minibatch loss at step 30000: 936.090271\n",
      "Minibatch accuracy: 34.60623574927465\n",
      "Minibatch loss at step 30100: 896.344238\n",
      "Minibatch accuracy: 35.818304367102236\n",
      "Minibatch loss at step 30200: 952.189819\n",
      "Minibatch accuracy: 35.4335831995492\n",
      "Minibatch loss at step 30300: 962.014282\n",
      "Minibatch accuracy: 36.21925218051341\n",
      "Minibatch loss at step 30400: 943.489319\n",
      "Minibatch accuracy: 36.247673012596614\n",
      "Minibatch loss at step 30500: 842.162842\n",
      "Minibatch accuracy: 35.44348632197812\n",
      "Minibatch loss at step 30600: 920.996948\n",
      "Minibatch accuracy: 33.20875687530512\n",
      "Minibatch loss at step 30700: 842.217346\n",
      "Minibatch accuracy: 34.60915102918501\n",
      "Minibatch loss at step 30800: 1000.520386\n",
      "Minibatch accuracy: 36.15066535305748\n",
      "Minibatch loss at step 30900: 878.877991\n",
      "Minibatch accuracy: 33.19723844877763\n",
      "Minibatch loss at step 31000: 971.643127\n",
      "Minibatch accuracy: 36.73722606970857\n",
      "Minibatch loss at step 31100: 869.151978\n",
      "Minibatch accuracy: 35.599732335831014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 31200: 960.460754\n",
      "Minibatch accuracy: 34.94970227989253\n",
      "Minibatch loss at step 31300: 772.446228\n",
      "Minibatch accuracy: 31.71750882731709\n",
      "Minibatch loss at step 31400: 982.858643\n",
      "Minibatch accuracy: 35.541064085479\n",
      "Minibatch loss at step 31500: 777.185059\n",
      "Minibatch accuracy: 31.653174594565748\n",
      "Minibatch loss at step 31600: 961.139160\n",
      "Minibatch accuracy: 36.32764952645832\n",
      "Minibatch loss at step 31700: 899.585815\n",
      "Minibatch accuracy: 35.36204350845338\n",
      "Minibatch loss at step 31800: 951.946960\n",
      "Minibatch accuracy: 34.841059313150915\n",
      "Minibatch loss at step 31900: 922.012512\n",
      "Minibatch accuracy: 34.41948471047141\n",
      "Minibatch loss at step 32000: 918.235779\n",
      "Minibatch accuracy: 34.158580089781246\n",
      "Minibatch loss at step 32100: 793.365356\n",
      "Minibatch accuracy: 32.69646148668079\n",
      "Minibatch loss at step 32200: 974.612427\n",
      "Minibatch accuracy: 35.98530448213374\n",
      "Minibatch loss at step 32300: 942.837280\n",
      "Minibatch accuracy: 36.534968934954975\n",
      "Minibatch loss at step 32400: 894.241455\n",
      "Minibatch accuracy: 35.55359826528638\n",
      "Minibatch loss at step 32500: 959.833557\n",
      "Minibatch accuracy: 35.59087764145558\n",
      "Minibatch loss at step 32600: 915.186584\n",
      "Minibatch accuracy: 34.51326065634501\n",
      "Minibatch loss at step 32700: 796.477966\n",
      "Minibatch accuracy: 32.3684989853175\n",
      "Minibatch loss at step 32800: 881.717285\n",
      "Minibatch accuracy: 33.326883327784344\n",
      "Minibatch loss at step 32900: 1009.634338\n",
      "Minibatch accuracy: 37.922092281515965\n",
      "Minibatch loss at step 33000: 841.254150\n",
      "Minibatch accuracy: 35.43405861266283\n",
      "Minibatch loss at step 33100: 1014.488281\n",
      "Minibatch accuracy: 37.10418713737036\n",
      "Minibatch loss at step 33200: 998.127625\n",
      "Minibatch accuracy: 35.916738228309896\n",
      "Minibatch loss at step 33300: 875.585876\n",
      "Minibatch accuracy: 34.0487698101699\n",
      "Minibatch loss at step 33400: 881.747314\n",
      "Minibatch accuracy: 33.94547702060247\n",
      "Minibatch loss at step 33500: 1003.116089\n",
      "Minibatch accuracy: 37.57134489250085\n",
      "Minibatch loss at step 33600: 851.657471\n",
      "Minibatch accuracy: 35.61826279496587\n",
      "Minibatch loss at step 33700: 965.284241\n",
      "Minibatch accuracy: 36.784047658045665\n",
      "Minibatch loss at step 33800: 1059.229980\n",
      "Minibatch accuracy: 37.56144380274639\n",
      "Minibatch loss at step 33900: 853.583862\n",
      "Minibatch accuracy: 33.74037406130636\n",
      "Minibatch loss at step 34000: 921.027771\n",
      "Minibatch accuracy: 35.53308283256517\n",
      "Minibatch loss at step 34100: 973.840881\n",
      "Minibatch accuracy: 36.12403018270006\n",
      "Minibatch loss at step 34200: 904.494751\n",
      "Minibatch accuracy: 34.18865150311668\n",
      "Minibatch loss at step 34300: 993.960449\n",
      "Minibatch accuracy: 36.4182433973474\n",
      "Minibatch loss at step 34400: 1075.663086\n",
      "Minibatch accuracy: 37.87285021229753\n",
      "Minibatch loss at step 34500: 920.501587\n",
      "Minibatch accuracy: 36.01157616883857\n",
      "Minibatch loss at step 34600: 995.199890\n",
      "Minibatch accuracy: 36.88873221531373\n",
      "Minibatch loss at step 34700: 898.525757\n",
      "Minibatch accuracy: 34.45987745620804\n",
      "Minibatch loss at step 34800: 918.162231\n",
      "Minibatch accuracy: 35.30393632333192\n",
      "Minibatch loss at step 34900: 992.255249\n",
      "Minibatch accuracy: 36.44783703073133\n",
      "Minibatch loss at step 35000: 1029.300537\n",
      "Minibatch accuracy: 38.31945319500348\n",
      "Minibatch loss at step 35100: 895.303650\n",
      "Minibatch accuracy: 36.04563893090952\n",
      "Minibatch loss at step 35200: 971.990112\n",
      "Minibatch accuracy: 37.0009056186678\n",
      "Minibatch loss at step 35300: 901.908386\n",
      "Minibatch accuracy: 34.43443902287814\n",
      "Minibatch loss at step 35400: 887.193665\n",
      "Minibatch accuracy: 34.89981825816755\n",
      "Minibatch loss at step 35500: 963.440796\n",
      "Minibatch accuracy: 35.08976352023835\n",
      "Minibatch loss at step 35600: 989.339722\n",
      "Minibatch accuracy: 36.35552226310932\n",
      "Minibatch loss at step 35700: 904.083191\n",
      "Minibatch accuracy: 35.11760202063471\n",
      "Minibatch loss at step 35800: 877.108154\n",
      "Minibatch accuracy: 35.50420859736209\n",
      "Minibatch loss at step 35900: 907.672546\n",
      "Minibatch accuracy: 33.70454114433247\n",
      "Minibatch loss at step 36000: 856.244019\n",
      "Minibatch accuracy: 35.4625734572512\n",
      "Minibatch loss at step 36100: 944.488770\n",
      "Minibatch accuracy: 35.330236143245635\n",
      "Minibatch loss at step 36200: 975.902344\n",
      "Minibatch accuracy: 36.046631176028434\n",
      "Minibatch loss at step 36300: 909.712708\n",
      "Minibatch accuracy: 34.76334613317596\n",
      "Minibatch loss at step 36400: 856.046509\n",
      "Minibatch accuracy: 35.39156835111383\n",
      "Minibatch loss at step 36500: 974.179077\n",
      "Minibatch accuracy: 34.37469992766755\n",
      "Minibatch loss at step 36600: 810.634644\n",
      "Minibatch accuracy: 32.837494200846464\n",
      "Minibatch loss at step 36700: 967.634644\n",
      "Minibatch accuracy: 35.37006680763499\n",
      "Minibatch loss at step 36800: 856.497986\n",
      "Minibatch accuracy: 32.460560504730196\n",
      "Minibatch loss at step 36900: 981.761169\n",
      "Minibatch accuracy: 36.62235351999855\n",
      "Minibatch loss at step 37000: 862.819519\n",
      "Minibatch accuracy: 35.00797900122199\n",
      "Minibatch loss at step 37100: 968.706055\n",
      "Minibatch accuracy: 34.451707737289674\n",
      "Minibatch loss at step 37200: 829.252930\n",
      "Minibatch accuracy: 32.8113095022128\n",
      "Minibatch loss at step 37300: 980.036377\n",
      "Minibatch accuracy: 35.591880849736995\n",
      "Minibatch loss at step 37400: 792.471191\n",
      "Minibatch accuracy: 32.086273243617164\n",
      "Minibatch loss at step 37500: 1012.625671\n",
      "Minibatch accuracy: 37.15999179540242\n",
      "Minibatch loss at step 37600: 935.863159\n",
      "Minibatch accuracy: 36.26187507540633\n",
      "Minibatch loss at step 37700: 924.697449\n",
      "Minibatch accuracy: 35.45279445637008\n",
      "Minibatch loss at step 37800: 960.700256\n",
      "Minibatch accuracy: 34.25932775297755\n",
      "Minibatch loss at step 37900: 912.900696\n",
      "Minibatch accuracy: 34.43925989434605\n",
      "Minibatch loss at step 38000: 794.305481\n",
      "Minibatch accuracy: 32.300321840743706\n",
      "Minibatch loss at step 38100: 997.366394\n",
      "Minibatch accuracy: 36.164497087743044\n",
      "Minibatch loss at step 38200: 978.326294\n",
      "Minibatch accuracy: 37.438041262781766\n",
      "Minibatch loss at step 38300: 832.692810\n",
      "Minibatch accuracy: 34.773799951773135\n",
      "Minibatch loss at step 38400: 1011.956116\n",
      "Minibatch accuracy: 37.50388326247863\n",
      "Minibatch loss at step 38500: 928.357483\n",
      "Minibatch accuracy: 34.88144681677138\n",
      "Minibatch loss at step 38600: 807.906555\n",
      "Minibatch accuracy: 33.118329222180726\n",
      "Minibatch loss at step 38700: 839.439514\n",
      "Minibatch accuracy: 32.85975763567041\n",
      "Minibatch loss at step 38800: 1000.091919\n",
      "Minibatch accuracy: 37.586779473784894\n",
      "Minibatch loss at step 38900: 848.809753\n",
      "Minibatch accuracy: 35.86761547473919\n",
      "Minibatch loss at step 39000: 983.527161\n",
      "Minibatch accuracy: 36.502340999413995\n",
      "Minibatch loss at step 39100: 996.646851\n",
      "Minibatch accuracy: 36.44789396672152\n",
      "Minibatch loss at step 39200: 883.820251\n",
      "Minibatch accuracy: 34.58133034171943\n",
      "Minibatch loss at step 39300: 835.674255\n",
      "Minibatch accuracy: 33.34869559738244\n",
      "Minibatch loss at step 39400: 989.790955\n",
      "Minibatch accuracy: 36.71410708678188\n",
      "Minibatch loss at step 39500: 948.319336\n",
      "Minibatch accuracy: 35.074610068885654\n",
      "Minibatch loss at step 39600: 933.937195\n",
      "Minibatch accuracy: 36.13828083621127\n",
      "Minibatch loss at step 39700: 1068.223633\n",
      "Minibatch accuracy: 37.61165019603301\n",
      "Minibatch loss at step 39800: 857.243347\n",
      "Minibatch accuracy: 34.5209313627028\n",
      "Minibatch loss at step 39900: 926.904053\n",
      "Minibatch accuracy: 35.653175535725424\n",
      "Minibatch loss at step 40000: 915.763550\n",
      "Minibatch accuracy: 34.95577213936741\n",
      "Minibatch loss at step 40100: 907.418274\n",
      "Minibatch accuracy: 34.37182691320847\n",
      "Minibatch loss at step 40200: 948.225525\n",
      "Minibatch accuracy: 35.895961047606036\n",
      "Minibatch loss at step 40300: 1074.156738\n",
      "Minibatch accuracy: 38.454128136041845\n",
      "Minibatch loss at step 40400: 958.462097\n",
      "Minibatch accuracy: 36.960672458624856\n",
      "Minibatch loss at step 40500: 990.180359\n",
      "Minibatch accuracy: 36.85863977647405\n",
      "Minibatch loss at step 40600: 935.953064\n",
      "Minibatch accuracy: 34.61151940309216\n",
      "Minibatch loss at step 40700: 892.542358\n",
      "Minibatch accuracy: 34.63634318936181\n",
      "Minibatch loss at step 40800: 966.270142\n",
      "Minibatch accuracy: 35.95586147778099\n",
      "Minibatch loss at step 40900: 1043.726318\n",
      "Minibatch accuracy: 37.88028531604534\n",
      "Minibatch loss at step 41000: 916.933899\n",
      "Minibatch accuracy: 35.70065295052389\n",
      "Minibatch loss at step 41100: 954.997070\n",
      "Minibatch accuracy: 36.984317239604245\n",
      "Minibatch loss at step 41200: 928.934570\n",
      "Minibatch accuracy: 34.57750013556503\n",
      "Minibatch loss at step 41300: 899.955383\n",
      "Minibatch accuracy: 35.63737449556996\n",
      "Minibatch loss at step 41400: 965.733459\n",
      "Minibatch accuracy: 35.82622719025128\n",
      "Minibatch loss at step 41500: 967.753845\n",
      "Minibatch accuracy: 36.32637764614888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 41600: 962.103577\n",
      "Minibatch accuracy: 36.343612290151825\n",
      "Minibatch loss at step 41700: 866.653687\n",
      "Minibatch accuracy: 35.77568529327041\n",
      "Minibatch loss at step 41800: 901.010132\n",
      "Minibatch accuracy: 33.042286915547706\n",
      "Minibatch loss at step 41900: 853.739502\n",
      "Minibatch accuracy: 34.69605750297863\n",
      "Minibatch loss at step 42000: 1003.912720\n",
      "Minibatch accuracy: 36.167227698221495\n",
      "Minibatch loss at step 42100: 895.559631\n",
      "Minibatch accuracy: 33.61350981364766\n",
      "Minibatch loss at step 42200: 971.238403\n",
      "Minibatch accuracy: 36.11623352758078\n",
      "Minibatch loss at step 42300: 885.600586\n",
      "Minibatch accuracy: 35.99593415082633\n",
      "Minibatch loss at step 42400: 949.960693\n",
      "Minibatch accuracy: 34.602410063846165\n",
      "Minibatch loss at step 42500: 784.334900\n",
      "Minibatch accuracy: 32.187829943697274\n",
      "Minibatch loss at step 42600: 992.705750\n",
      "Minibatch accuracy: 35.87927348240206\n",
      "Minibatch loss at step 42700: 784.862610\n",
      "Minibatch accuracy: 31.680981208171254\n",
      "Minibatch loss at step 42800: 965.646973\n",
      "Minibatch accuracy: 36.443683812750734\n",
      "Minibatch loss at step 42900: 918.979797\n",
      "Minibatch accuracy: 35.75394359258675\n",
      "Minibatch loss at step 43000: 956.415588\n",
      "Minibatch accuracy: 34.83188909672967\n",
      "Minibatch loss at step 43100: 906.054016\n",
      "Minibatch accuracy: 33.91277478982386\n",
      "Minibatch loss at step 43200: 921.129517\n",
      "Minibatch accuracy: 34.18766781837239\n",
      "Minibatch loss at step 43300: 794.270691\n",
      "Minibatch accuracy: 32.23362896010159\n",
      "Minibatch loss at step 43400: 968.756958\n",
      "Minibatch accuracy: 35.790014960857974\n",
      "Minibatch loss at step 43500: 955.090210\n",
      "Minibatch accuracy: 36.428688096145464\n",
      "Minibatch loss at step 43600: 904.737122\n",
      "Minibatch accuracy: 35.85566089153497\n",
      "Minibatch loss at step 43700: 952.785706\n",
      "Minibatch accuracy: 34.98191671521606\n",
      "Minibatch loss at step 43800: 893.110352\n",
      "Minibatch accuracy: 34.277234909015235\n",
      "Minibatch loss at step 43900: 788.452148\n",
      "Minibatch accuracy: 32.418648182797504\n",
      "Minibatch loss at step 44000: 892.974731\n",
      "Minibatch accuracy: 33.675536343863556\n",
      "Minibatch loss at step 44100: 989.164856\n",
      "Minibatch accuracy: 37.55640777082437\n",
      "Minibatch loss at step 44200: 851.483459\n",
      "Minibatch accuracy: 35.292086536082174\n",
      "Minibatch loss at step 44300: 1018.919189\n",
      "Minibatch accuracy: 37.349414450103836\n",
      "Minibatch loss at step 44400: 997.107178\n",
      "Minibatch accuracy: 36.169551423750235\n",
      "Minibatch loss at step 44500: 871.219482\n",
      "Minibatch accuracy: 34.086214150550475\n",
      "Minibatch loss at step 44600: 884.901978\n",
      "Minibatch accuracy: 34.04586929159414\n",
      "Minibatch loss at step 44700: 992.900635\n",
      "Minibatch accuracy: 37.639635857696604\n",
      "Minibatch loss at step 44800: 850.869141\n",
      "Minibatch accuracy: 35.57173924356947\n",
      "Minibatch loss at step 44900: 985.519287\n",
      "Minibatch accuracy: 37.062707498365185\n",
      "Minibatch loss at step 45000: 1058.224976\n",
      "Minibatch accuracy: 37.84792697324221\n",
      "Minibatch loss at step 45100: 853.240662\n",
      "Minibatch accuracy: 33.68826276096116\n",
      "Minibatch loss at step 45200: 903.372009\n",
      "Minibatch accuracy: 34.95108537857766\n",
      "Minibatch loss at step 45300: 955.407410\n",
      "Minibatch accuracy: 36.1621139637622\n",
      "Minibatch loss at step 45400: 908.707153\n",
      "Minibatch accuracy: 33.99132112921436\n",
      "Minibatch loss at step 45500: 1004.783997\n",
      "Minibatch accuracy: 36.71817153799669\n",
      "Minibatch loss at step 45600: 1079.395996\n",
      "Minibatch accuracy: 38.06441138468451\n",
      "Minibatch loss at step 45700: 906.587341\n",
      "Minibatch accuracy: 35.782616308520716\n",
      "Minibatch loss at step 45800: 1017.549438\n",
      "Minibatch accuracy: 37.5353609322263\n",
      "Minibatch loss at step 45900: 917.914001\n",
      "Minibatch accuracy: 34.574643972995226\n",
      "Minibatch loss at step 46000: 947.056824\n",
      "Minibatch accuracy: 35.714482944488886\n",
      "Minibatch loss at step 46100: 988.364929\n",
      "Minibatch accuracy: 36.450084260233545\n",
      "Minibatch loss at step 46200: 1027.587646\n",
      "Minibatch accuracy: 38.0516023115527\n",
      "Minibatch loss at step 46300: 891.389587\n",
      "Minibatch accuracy: 36.30062467179456\n",
      "Minibatch loss at step 46400: 986.588989\n",
      "Minibatch accuracy: 37.01618232484611\n",
      "Minibatch loss at step 46500: 905.533386\n",
      "Minibatch accuracy: 34.37995702895075\n",
      "Minibatch loss at step 46600: 896.321533\n",
      "Minibatch accuracy: 34.97963274580295\n",
      "Minibatch loss at step 46700: 960.401367\n",
      "Minibatch accuracy: 35.16424735426862\n",
      "Minibatch loss at step 46800: 1005.478455\n",
      "Minibatch accuracy: 36.656709555942825\n",
      "Minibatch loss at step 46900: 879.974121\n",
      "Minibatch accuracy: 34.55397301007874\n",
      "Minibatch loss at step 47000: 852.095215\n",
      "Minibatch accuracy: 34.74858957625565\n",
      "Minibatch loss at step 47100: 897.396179\n",
      "Minibatch accuracy: 33.79377225124106\n",
      "Minibatch loss at step 47200: 886.135925\n",
      "Minibatch accuracy: 36.07460598905586\n",
      "Minibatch loss at step 47300: 940.776062\n",
      "Minibatch accuracy: 35.07598476210254\n",
      "Minibatch loss at step 47400: 963.942688\n",
      "Minibatch accuracy: 35.555843648803005\n",
      "Minibatch loss at step 47500: 915.919434\n",
      "Minibatch accuracy: 35.36020181037309\n",
      "Minibatch loss at step 47600: 855.453796\n",
      "Minibatch accuracy: 35.45190610509363\n",
      "Minibatch loss at step 47700: 949.240601\n",
      "Minibatch accuracy: 33.640508882753615\n",
      "Minibatch loss at step 47800: 802.490417\n",
      "Minibatch accuracy: 33.003149633666595\n",
      "Minibatch loss at step 47900: 971.340759\n",
      "Minibatch accuracy: 35.505097359063285\n",
      "Minibatch loss at step 48000: 859.683533\n",
      "Minibatch accuracy: 32.59945719448408\n",
      "Minibatch loss at step 48100: 997.939575\n",
      "Minibatch accuracy: 37.02963472215531\n",
      "Minibatch loss at step 48200: 864.562500\n",
      "Minibatch accuracy: 34.988582819748146\n",
      "Minibatch loss at step 48300: 983.387146\n",
      "Minibatch accuracy: 34.72432940654686\n",
      "Minibatch loss at step 48400: 807.760498\n",
      "Minibatch accuracy: 32.25155375678143\n",
      "Minibatch loss at step 48500: 965.847107\n",
      "Minibatch accuracy: 35.44206388711195\n",
      "Minibatch loss at step 48600: 801.990356\n",
      "Minibatch accuracy: 32.14863020142732\n",
      "Minibatch loss at step 48700: 1001.501099\n",
      "Minibatch accuracy: 37.12428976593357\n",
      "Minibatch loss at step 48800: 920.962341\n",
      "Minibatch accuracy: 36.06120353497128\n",
      "Minibatch loss at step 48900: 940.052734\n",
      "Minibatch accuracy: 35.57977529659582\n",
      "Minibatch loss at step 49000: 962.863586\n",
      "Minibatch accuracy: 34.61625390159495\n",
      "Minibatch loss at step 49100: 945.947632\n",
      "Minibatch accuracy: 35.27618431303902\n",
      "Minibatch loss at step 49200: 787.892456\n",
      "Minibatch accuracy: 32.7015385723669\n",
      "Minibatch loss at step 49300: 981.110474\n",
      "Minibatch accuracy: 35.97146153441468\n",
      "Minibatch loss at step 49400: 954.289612\n",
      "Minibatch accuracy: 36.94309787319886\n",
      "Minibatch loss at step 49500: 870.778687\n",
      "Minibatch accuracy: 35.68581363682038\n",
      "Minibatch loss at step 49600: 1022.823181\n",
      "Minibatch accuracy: 37.46772374011564\n",
      "Minibatch loss at step 49700: 939.519104\n",
      "Minibatch accuracy: 35.03678312680832\n",
      "Minibatch loss at step 49800: 787.997986\n",
      "Minibatch accuracy: 32.798175816866134\n",
      "Minibatch loss at step 49900: 836.712097\n",
      "Minibatch accuracy: 32.7758705518204\n",
      "Minibatch loss at step 50000: 1018.784729\n",
      "Minibatch accuracy: 38.204276887031085\n",
      "Test accuracy: 35.35903667535889\n",
      "=== TEST MEAN ERROR (DEGREE) ===\n",
      "PITCH: 52.4070109018504\n",
      "YAW: 97.39784419464037\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Starting Training...\")\n",
    "    print(\"Train dataset shape: \", training_dataset.shape)\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data\n",
    "        offset = (step * batch_size) % (training_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = training_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = training_label[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \" + str(accuracy(predictions, batch_labels)))\n",
    "#             print(\"Validation accuracy: \" + str(accuracy(valid_prediction.eval(), validation_label, True)))      \n",
    "  \n",
    "    saver.save(session, './dnn_1600i_4h_3o', global_step=step) #save the session    \n",
    "    print(\"Test accuracy: \" + str(accuracy(test_prediction.eval(), test_label)))\n",
    "    final_test(test_prediction.eval(), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
